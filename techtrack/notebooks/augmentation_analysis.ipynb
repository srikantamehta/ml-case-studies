{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation Analysis\n",
    "\n",
    "Here we chose the 10 images that were determined as the top 10 hard negatives in the hard mining analysis and used these images for augmentation. All six augmentations were applied to each image and resulting images and corrected yolo annotations are saved in storage/output_augmentations. We then use model 2 to predict on these images and use the hard mining class to provide their loss score again so we can compare to their scores in hard mining analyisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append system path\n",
    "import sys, os\n",
    "import cv2 as cv\n",
    "sys.path.append(os.path.abspath('../inference'))\n",
    "sys.path.append(os.path.abspath('../rectification'))\n",
    "\n",
    "from augmentation import Augmentation\n",
    "\n",
    "output_dir = \"../storage/output_augmentations\"\n",
    "augmenter = Augmentation(output_dir)\n",
    "\n",
    "# Import Model and NMS classes\n",
    "from object_detection import Model\n",
    "\n",
    "model_2 = Model(weights_file=\"../storage/yolo_model_2/yolov4-tiny-logistics_size_416_2.weights\", \n",
    "                config_file=\"../storage/yolo_model_2/yolov4-tiny-logistics_size_416_2.cfg\", \n",
    "                names_file=\"../storage/yolo_model_2/logistics.names\")\n",
    "\n",
    "image_filenames = {\n",
    "    '../storage/logistics/387a749f761852d0_jpg.rf.00080ecac3fb871dc5f936dab4fe9d80.jpg',\n",
    "    '../storage/logistics/EAN13_09_0088_jpg.rf.2031e0c46b10aaa695a811ce27f1ef56.jpg',\n",
    "    '../storage/logistics/Img_web_555_jpg.rf.29fb8649096f57f40d9456b2ab6bd8c1.jpg',\n",
    "    '../storage/logistics/621-jpg__jpg_jpg.rf.23ce21d5f284132e5a37ea742622b97c.jpg',\n",
    "    '../storage/logistics/007118_jpg.rf.08eb2afd28293afc0f53474348b3ccab.jpg',\n",
    "    '../storage/logistics/LDR08AW1FPEB_jpg.rf.c23f27058cde6dd65c8f29d62d9a8e37.jpg',\n",
    "    '../storage/logistics/LDR08AW1FPEB_jpg.rf.321e7a024d64f32232980dbb1ddcab3c.jpg',\n",
    "    '../storage/logistics/WEJCF41ZFDDZ_jpg.rf.e72efe631ba5047517bc8012cbb302f6.jpg',\n",
    "    '../storage/logistics/fire1_mp4-313_jpg.rf.63396291379b3c704d855a3d9ce07e6b.jpg',\n",
    "    '../storage/logistics/74_resized_jpg.rf.da320b1886aebd8a6dac9fd79ca3578c.jpg'\n",
    "}\n",
    "\n",
    "yolo_filenames = {\n",
    "    '../storage/logistics/387a749f761852d0_jpg.rf.00080ecac3fb871dc5f936dab4fe9d80.txt',\n",
    "    '../storage/logistics/EAN13_09_0088_jpg.rf.2031e0c46b10aaa695a811ce27f1ef56.txt',\n",
    "    '../storage/logistics/Img_web_555_jpg.rf.29fb8649096f57f40d9456b2ab6bd8c1.txt',\n",
    "    '../storage/logistics/621-jpg__jpg_jpg.rf.23ce21d5f284132e5a37ea742622b97c.txt',\n",
    "    '../storage/logistics/007118_jpg.rf.08eb2afd28293afc0f53474348b3ccab.txt',\n",
    "    '../storage/logistics/LDR08AW1FPEB_jpg.rf.c23f27058cde6dd65c8f29d62d9a8e37.txt',\n",
    "    '../storage/logistics/LDR08AW1FPEB_jpg.rf.321e7a024d64f32232980dbb1ddcab3c.txt',\n",
    "    '../storage/logistics/WEJCF41ZFDDZ_jpg.rf.e72efe631ba5047517bc8012cbb302f6.txt',\n",
    "    '../storage/logistics/fire1_mp4-313_jpg.rf.63396291379b3c704d855a3d9ce07e6b.txt',\n",
    "    '../storage/logistics/74_resized_jpg.rf.da320b1886aebd8a6dac9fd79ca3578c.txt'\n",
    "}\n",
    "\n",
    "for image_filename, yolo_filename in zip(image_filenames, yolo_filenames):\n",
    "    augmenter.apply_augmentations(image_filename, yolo_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the test images and labels\n",
    "image_dir = \"../storage/output_augmentations/\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "image_paths.sort(key=lambda x: os.path.splitext(os.path.basename(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '../storage/augmented_predictions'\n",
    "for image_path in image_paths:\n",
    "    # Extract just the filename without the extension\n",
    "    image_filename = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    # Read the image\n",
    "    image = cv.imread(image_path)\n",
    "    image_height, image_width = image.shape[:2]\n",
    "    outputs, original_size = model_2.predict(image)\n",
    "    model_2.save_predictions(outputs, original_size, output_path, image_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hard_negative_mining import HardNegativeMining\n",
    "\n",
    "hm = HardNegativeMining(iou_threshold=0.5, lambda_bb=1, lambda_obj=1, lambda_cls=1, lambda_no_obj=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dir = '../storage/augmented_predictions/'\n",
    "prediction_paths = [os.path.join(prediction_dir, f) for f in os.listdir(prediction_dir) if f.endswith('.txt')]\n",
    "prediction_paths.sort(key=lambda x: os.path.splitext(os.path.basename(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sample_hard_negatives(prediction_dir, annotation_image_dir, num_samples, hm):\n",
    "    \"\"\"\n",
    "    Test the sample_hard_negatives method from the HardNegativeMining class and return the corresponding images with losses.\n",
    "    \n",
    "    :param prediction_dir: Directory containing predicted YOLO bounding boxes (.txt).\n",
    "    :param annotation_image_dir: Directory containing both the annotations (.txt) and the images (.jpg).\n",
    "    :param num_samples: Number of hard negatives to return.\n",
    "    :param hm: Initialized HardNegativeMining object.\n",
    "    \n",
    "    :return: List of tuples (image_path, loss) for the top hard negatives.\n",
    "    \"\"\"\n",
    "    hard_negatives = hm.sample_hard_negatives(prediction_dir, annotation_image_dir, num_samples)\n",
    "    \n",
    "    # Prepare list to store the image paths and their corresponding losses\n",
    "    results = []\n",
    "\n",
    "    for pred_file, loss in hard_negatives:\n",
    "        # Get the image file name from the prediction file (replace .txt with .jpg)\n",
    "        image_filename = os.path.splitext(pred_file)[0] + '.jpg'\n",
    "        image_path = os.path.join(annotation_image_dir, image_filename)\n",
    "        \n",
    "        # Check if the image file exists\n",
    "        if os.path.exists(image_path):\n",
    "            results.append((image_path, loss))\n",
    "        else:\n",
    "            print(f\"Warning: Image {image_filename} does not exist.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# Obtain hard negative results\n",
    "hard_negatives_results = test_sample_hard_negatives(\n",
    "    prediction_dir='../storage/augmented_predictions/',\n",
    "    annotation_image_dir='../storage/output_augmentations/',  \n",
    "    num_samples=len(prediction_paths),  \n",
    "    hm=hm  # HardNegativeMining object\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores from the hard mining analysis module:\n",
    "\n",
    "```\n",
    "Image: ../storage/logistics/387a749f761852d0_jpg.rf.00080ecac3fb871dc5f936dab4fe9d80.jpg, Loss: 22.000000\n",
    "Image: ../storage/logistics/EAN13_09_0088_jpg.rf.2031e0c46b10aaa695a811ce27f1ef56.jpg, Loss: 21.375287\n",
    "Image: ../storage/logistics/Img_web_555_jpg.rf.29fb8649096f57f40d9456b2ab6bd8c1.jpg, Loss: 21.365474\n",
    "Image: ../storage/logistics/621-jpg__jpg_jpg.rf.23ce21d5f284132e5a37ea742622b97c.jpg, Loss: 21.335702\n",
    "Image: ../storage/logistics/007118_jpg.rf.08eb2afd28293afc0f53474348b3ccab.jpg, Loss: 21.331186\n",
    "Image: ../storage/logistics/LDR08AW1FPEB_jpg.rf.c23f27058cde6dd65c8f29d62d9a8e37.jpg, Loss: 21.320602\n",
    "Image: ../storage/logistics/LDR08AW1FPEB_jpg.rf.321e7a024d64f32232980dbb1ddcab3c.jpg, Loss: 21.297516\n",
    "Image: ../storage/logistics/WEJCF41ZFDDZ_jpg.rf.e72efe631ba5047517bc8012cbb302f6.jpg, Loss: 21.289732\n",
    "Image: ../storage/logistics/fire1_mp4-313_jpg.rf.63396291379b3c704d855a3d9ce07e6b.jpg, Loss: 21.245449\n",
    "Image: ../storage/logistics/74_resized_jpg.rf.da320b1886aebd8a6dac9fd79ca3578c.jpg, Loss: 21.233575 \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: ../storage/output_augmentations/387a749f761852d0_jpg_brightness.jpg, Loss: 22.000000\n",
      "Image: ../storage/output_augmentations/387a749f761852d0_jpg_contrast.jpg, Loss: 22.000000\n",
      "Image: ../storage/output_augmentations/387a749f761852d0_jpg_resized.jpg, Loss: 22.000000\n",
      "Image: ../storage/output_augmentations/387a749f761852d0_jpg_blurred.jpg, Loss: 22.000000\n",
      "Image: ../storage/output_augmentations/LDR08AW1FPEB_jpg_flipped.jpg, Loss: 21.376539\n",
      "Image: ../storage/output_augmentations/EAN13_09_0088_jpg_brightness.jpg, Loss: 21.375227\n",
      "Image: ../storage/output_augmentations/621-jpg__jpg_jpg_resized.jpg, Loss: 21.363238\n",
      "Image: ../storage/output_augmentations/621-jpg__jpg_jpg_contrast.jpg, Loss: 21.353002\n",
      "Image: ../storage/output_augmentations/LDR08AW1FPEB_jpg_resized.jpg, Loss: 21.325368\n",
      "Image: ../storage/output_augmentations/74_resized_jpg_blurred.jpg, Loss: 21.306260\n",
      "Image: ../storage/output_augmentations/Img_web_555_jpg_flipped.jpg, Loss: 21.305775\n",
      "Image: ../storage/output_augmentations/EAN13_09_0088_jpg_contrast.jpg, Loss: 21.295660\n",
      "Image: ../storage/output_augmentations/LDR08AW1FPEB_jpg_contrast.jpg, Loss: 21.270625\n",
      "Image: ../storage/output_augmentations/EAN13_09_0088_jpg_flipped.jpg, Loss: 21.266056\n",
      "Image: ../storage/output_augmentations/74_resized_jpg_contrast.jpg, Loss: 21.264519\n",
      "Image: ../storage/output_augmentations/74_resized_jpg_brightness.jpg, Loss: 21.247362\n",
      "Image: ../storage/output_augmentations/74_resized_jpg_flipped.jpg, Loss: 21.219100\n",
      "Image: ../storage/output_augmentations/74_resized_jpg_resized.jpg, Loss: 21.186648\n",
      "Image: ../storage/output_augmentations/007118_jpg_resized.jpg, Loss: 21.185107\n",
      "Image: ../storage/output_augmentations/007118_jpg_brightness.jpg, Loss: 21.154316\n",
      "Image: ../storage/output_augmentations/74_resized_jpg_rotated.jpg, Loss: 21.152685\n",
      "Image: ../storage/output_augmentations/007118_jpg_contrast.jpg, Loss: 11.587123\n",
      "Image: ../storage/output_augmentations/fire1_mp4-313_jpg_flipped.jpg, Loss: 10.733030\n",
      "Image: ../storage/output_augmentations/007118_jpg_flipped.jpg, Loss: 6.725861\n",
      "Image: ../storage/output_augmentations/007118_jpg_blurred.jpg, Loss: 2.050657\n",
      "Image: ../storage/output_augmentations/621-jpg__jpg_jpg_rotated.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/EAN13_09_0088_jpg_resized.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/EAN13_09_0088_jpg_rotated.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/LDR08AW1FPEB_jpg_brightness.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/WEJCF41ZFDDZ_jpg_contrast.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/621-jpg__jpg_jpg_flipped.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/Img_web_555_jpg_blurred.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/LDR08AW1FPEB_jpg_rotated.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/WEJCF41ZFDDZ_jpg_blurred.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/fire1_mp4-313_jpg_blurred.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/Img_web_555_jpg_contrast.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/621-jpg__jpg_jpg_blurred.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/EAN13_09_0088_jpg_blurred.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/fire1_mp4-313_jpg_brightness.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/621-jpg__jpg_jpg_brightness.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/Img_web_555_jpg_brightness.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/Img_web_555_jpg_resized.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/WEJCF41ZFDDZ_jpg_flipped.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/LDR08AW1FPEB_jpg_blurred.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/WEJCF41ZFDDZ_jpg_resized.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/fire1_mp4-313_jpg_resized.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/fire1_mp4-313_jpg_rotated.jpg, Loss: 2.000000\n",
      "Image: ../storage/output_augmentations/Img_web_555_jpg_rotated.jpg, Loss: 0.259200\n",
      "Image: ../storage/output_augmentations/387a749f761852d0_jpg_flipped.jpg, Loss: 0.212753\n",
      "Image: ../storage/output_augmentations/007118_jpg_rotated.jpg, Loss: 0.204800\n",
      "Image: ../storage/output_augmentations/387a749f761852d0_jpg_rotated.jpg, Loss: 0.180000\n",
      "Image: ../storage/output_augmentations/WEJCF41ZFDDZ_jpg_rotated.jpg, Loss: 0.178800\n",
      "Image: ../storage/output_augmentations/fire1_mp4-313_jpg_contrast.jpg, Loss: 0.156800\n",
      "Image: ../storage/output_augmentations/WEJCF41ZFDDZ_jpg_brightness.jpg, Loss: 0.080000\n"
     ]
    }
   ],
   "source": [
    "# Print the image paths and their corresponding losses outside of the function\n",
    "for image_path, loss in hard_negatives_results:\n",
    "    print(f\"Image: {image_path}, Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the original losses to the losses of the augmented images, we can see that some augmentations were able to lower the losses significantly. The code below alows us to plot each augmented image with ground truth and predicted labels to further analyze. It has been commented out for submission to lower the notebook size. From the results above there does not seem to be a specific augmentations that performs the best. It seems like it really depends on the image what will work best and the reason for the high loss. It would be interesting to combine two or more augmentations on an image to see if different combinations can further improve performance on these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_image_with_boxes(image, bboxes, class_ids, scores, model, title=\"Predictions\"):\n",
    "    \"\"\"Function to display image with bounding boxes (converted from YOLO format).\"\"\"\n",
    "    h, w = image.shape[:2]  # Get image dimensions\n",
    "\n",
    "    for i, bbox in enumerate(bboxes):\n",
    "        cx, cy, bw, bh = bbox  # YOLO format: center_x, center_y, width, height (normalized)\n",
    "        \n",
    "        # Convert from YOLO format (normalized) to pixel coordinates\n",
    "        x1 = int((cx - bw / 2) * w)  # Top-left x\n",
    "        y1 = int((cy - bh / 2) * h)  # Top-left y\n",
    "        x2 = int((cx + bw / 2) * w)  # Bottom-right x\n",
    "        y2 = int((cy + bh / 2) * h)  # Bottom-right y\n",
    "\n",
    "        # Create label for the class and confidence score\n",
    "        label = f\"{model.classes[class_ids[i]]}: {scores[i]:.2f}\"\n",
    "        \n",
    "        # Draw the rectangle on the image\n",
    "        cv.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "        # Add the label text above the bounding box\n",
    "        cv.putText(image, label, (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Convert image to RGB for matplotlib display\n",
    "    image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Plot the image with bounding boxes\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")  # Hide axes\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def display_images_with_gt_and_predictions(hard_negatives_results, hm, model, annotation_image_dir, prediction_dir):\n",
    "    \"\"\"\n",
    "    Display images with ground truth and predicted bounding boxes labeled.\n",
    "    \n",
    "    :param hard_negatives_results: List of tuples (image_path, loss) from the sample_hard_negatives function.\n",
    "    :param hm: Initialized HardNegativeMining object.\n",
    "    :param model: The model object with class names to label the boxes.\n",
    "    :param annotation_image_dir: Directory containing both the annotations (.txt) and the images (.jpg).\n",
    "    :param prediction_dir: Directory containing predicted YOLO bounding boxes (.txt).\n",
    "    \"\"\"\n",
    "    for image_path, _ in hard_negatives_results:\n",
    "        image_filename = os.path.basename(image_path).replace('.jpg', '.txt')\n",
    "\n",
    "        # Read the image\n",
    "        image = cv.imread(image_path)\n",
    "        img_height, img_width = image.shape[:2]\n",
    "\n",
    "        # Read ground truth and predicted labels\n",
    "        annotation_file = os.path.join(annotation_image_dir, image_filename)  # Ground truth\n",
    "        prediction_file = os.path.join(prediction_dir, image_filename)  # Predictions\n",
    "\n",
    "        ground_truth_labels = hm.read_yolo_labels(annotation_file)\n",
    "        predicted_labels = hm.read_yolo_labels(prediction_file)\n",
    "\n",
    "        # Ground truth boxes and class IDs\n",
    "        gt_bboxes = [label[1:5] for label in ground_truth_labels]\n",
    "        gt_class_ids = [label[0] for label in ground_truth_labels]\n",
    "\n",
    "        # Predicted boxes, class IDs, and scores\n",
    "        pred_bboxes = [label[1:5] for label in predicted_labels]\n",
    "        pred_class_ids = [label[0] for label in predicted_labels]\n",
    "        pred_scores = [label[5] for label in predicted_labels]  # Confidence scores\n",
    "\n",
    "        # Display ground truth\n",
    "        print(f\"Displaying Ground Truth for Image: {image_path}\")\n",
    "        display_image_with_boxes(image.copy(), gt_bboxes, gt_class_ids, [1.0]*len(gt_class_ids), model, title=\"Ground Truth\")\n",
    "\n",
    "        # Display predictions\n",
    "        print(f\"Displaying Predictions for Image: {image_path}\")\n",
    "        display_image_with_boxes(image.copy(), pred_bboxes, pred_class_ids, pred_scores, model, title=\"Predictions\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Display the images with ground truth and predictions\n",
    "# display_images_with_gt_and_predictions(\n",
    "#     hard_negatives_results=hard_negatives_results, \n",
    "#     hm=hm, \n",
    "#     model=model_2, \n",
    "#     annotation_image_dir='../storage/output_augmentations/',  \n",
    "#     prediction_dir='../storage/augmented_predictions/'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
