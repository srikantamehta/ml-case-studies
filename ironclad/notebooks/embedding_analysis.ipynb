{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "from pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision_at_k(retrieved_items, true_name):\n",
    "    \"\"\"\n",
    "    Calculate the average precision at K for a single query.\n",
    "    \n",
    "    :param retrieved_items: List of items retrieved by the model (sorted by rank).\n",
    "    :param true_name: The true name of the probe image.\n",
    "    :return: Average precision at K.\n",
    "    \"\"\"\n",
    "    num_relevant = 0\n",
    "    cumulative_precision = 0.0\n",
    "\n",
    "    for rank, item in enumerate(retrieved_items, start=1):\n",
    "        if item['name'] == true_name:\n",
    "            num_relevant += 1\n",
    "            # Precision at this rank\n",
    "            precision_at_rank = num_relevant / rank\n",
    "            cumulative_precision += precision_at_rank\n",
    "\n",
    "    # Return average precision, normalized by the number of relevant items found\n",
    "    if num_relevant == 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return cumulative_precision / num_relevant\n",
    "\n",
    "def mean_average_precision(true_name_list, retrieved_items_list):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision (MAP) over all probe queries.\n",
    "    \n",
    "    :param true_name_list: List of true names (one for each query/probe image).\n",
    "    :param retrieved_items_list: List of retrieved items (list of lists for each query).\n",
    "    :return: Mean Average Precision (MAP) score.\n",
    "    \"\"\"\n",
    "    total_ap = 0.0\n",
    "    num_queries = len(true_name_list)\n",
    "\n",
    "    # Loop over all queries and calculate the average precision for each\n",
    "    for true_name, retrieved_items in zip(true_name_list, retrieved_items_list):\n",
    "        ap = average_precision_at_k(retrieved_items, true_name)\n",
    "        total_ap += ap\n",
    "\n",
    "    # Return the mean of the average precision scores\n",
    "    return total_ap / num_queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pipeline, probe_directory, gallery_faiss_path, gallery_metadata_path, k=5, metric='euclidean'):\n",
    "    \"\"\"\n",
    "    Evaluate the model by calculating Mean Average Precision (MAP) using all probe images.\n",
    "    \n",
    "    :param pipeline: The initialized Pipeline object (with the desired model).\n",
    "    :param probe_directory: Directory containing probe images.\n",
    "    :param gallery_faiss_path: Path to the precomputed FAISS index for the gallery.\n",
    "    :param gallery_metadata_path: Path to the metadata associated with the FAISS index.\n",
    "    :param k: Number of nearest neighbors to retrieve.\n",
    "    :param metric: Distance metric to use in FAISS search ('euclidean', 'minkowski', etc.).\n",
    "    :return: Mean Average Precision (MAP) for all probe images.\n",
    "    \"\"\"\n",
    "    true_name_list = []\n",
    "    retrieved_items_list = []\n",
    "    \n",
    "    for root, _, files in os.walk(probe_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "                probe_image_path = os.path.join(root, file)\n",
    "                \n",
    "                # Extract the true name from the folder name (assumed to be the name of the person)\n",
    "                true_name = os.path.basename(root)\n",
    "                true_name_list.append(true_name)\n",
    "                \n",
    "                # Perform a search with the pipeline and store the retrieved items\n",
    "                retrieved_items = pipeline.search_gallery(probe_image_path, k=k, metric=metric, faiss_path=gallery_faiss_path, metadata_path=gallery_metadata_path)\n",
    "                retrieved_items_list.append(retrieved_items)\n",
    "    \n",
    "    # Calculate Mean Average Precision (MAP)\n",
    "    map_score = mean_average_precision(true_name_list, retrieved_items_list)\n",
    "    \n",
    "    return map_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize the 'vggface2' pipeline and precompute gallery embeddings\n",
    "pipeline_vggface2 = Pipeline(pretrained='vggface2', index_type='brute_force')\n",
    "\n",
    "# # Precompute and save embeddings for 'vggface2'\n",
    "# gallery_directory = \"../storage/multi_image_gallery\"\n",
    "# pipeline_vggface2._Pipeline__precompute(gallery_directory)\n",
    "# pipeline_vggface2._Pipeline__save_embeddings(faiss_path=\"../storage/catalog/vggface2.index\", \n",
    "#                                              metadata_path=\"../storage/catalog/vggface2_metadata.pkl\")\n",
    "\n",
    "# Step 2: Initialize the 'casia-webface' pipeline and precompute gallery embeddings\n",
    "pipeline_casia_webface = Pipeline(pretrained='casia-webface', index_type='brute_force')\n",
    "\n",
    "# # Precompute and save embeddings for 'casia-webface'\n",
    "# pipeline_casia_webface._Pipeline__precompute(gallery_directory)\n",
    "# pipeline_casia_webface._Pipeline__save_embeddings(faiss_path=\"../storage/catalog/casia_webface.index\", \n",
    "#                                                   metadata_path=\"../storage/catalog/casia_webface_metadata.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model General Performance\n",
    "\n",
    "From the results, it's clear that the vggface2 model significantly outperforms casia-webface across all evaluations. For all distance metrics—euclidean, dot product, cosine, and minkowski—vggface2 achieved a consistent Mean Average Precision (MAP) of about 0.54, whereas casia-webface struggled, with a MAP consistently around 0.12. This suggests that vggface2 is much better suited for this particular task of face recognition.\n",
    "\n",
    "When analyzing the performance across different values of k (the number of nearest neighbors), we see a similar trend. For vggface2, the MAP peaks at k=5 with a score of 0.55, then slightly declines as k increases. Meanwhile, casia-webface shows much less variance across different k values, with its MAP remaining relatively low, peaking at k=15 with a score of 0.119. This consistent underperformance suggests that casia-webface struggles to retrieve relevant results effectively, even when given more neighbors to work with.\n",
    "\n",
    "Interestingly, changing the distance metric or the k value didn't significantly impact the results for either model. The vggface2 model maintained high performance across all metrics and k values, while casia-webface continued to perform poorly. This suggests that the vggface2 model's ability to retrieve and rank relevant faces is inherently more robust, independent of these factors.\n",
    "\n",
    "Overall, these results clearly indicate that vggface2 provides much stronger performance for the face recognition task at hand, regardless of the distance metric or k value. Since accuracy and precision are critical for this application, vggface2 would be the preferred model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating vggface2 model on all probe images...\n",
      "Evaluating vggface2 with euclidean distance...\n",
      "Mean Average Precision (MAP) for vggface2 using euclidean: 0.5445760627407453\n",
      "Evaluating vggface2 with dot_product distance...\n",
      "Mean Average Precision (MAP) for vggface2 using dot_product: 0.5445760627407453\n",
      "Evaluating vggface2 with cosine distance...\n",
      "Mean Average Precision (MAP) for vggface2 using cosine: 0.5445760627407453\n",
      "Evaluating vggface2 with minkowski distance...\n",
      "Mean Average Precision (MAP) for vggface2 using minkowski: 0.5445760627407453\n",
      "Evaluating casia-webface model on all probe images...\n",
      "Evaluating casia-webface with euclidean distance...\n",
      "Mean Average Precision (MAP) for casia-webface using euclidean: 0.1187389770723104\n",
      "Evaluating casia-webface with dot_product distance...\n",
      "Mean Average Precision (MAP) for casia-webface using dot_product: 0.1187389770723104\n",
      "Evaluating casia-webface with cosine distance...\n",
      "Mean Average Precision (MAP) for casia-webface using cosine: 0.1187389770723104\n",
      "Evaluating casia-webface with minkowski distance...\n",
      "Mean Average Precision (MAP) for casia-webface using minkowski: 0.1187389770723104\n"
     ]
    }
   ],
   "source": [
    "# Define the distance metrics to test\n",
    "distance_metrics = ['euclidean', 'dot_product', 'cosine', 'minkowski']\n",
    "\n",
    "# Evaluate the 'vggface2' model using all probe images and multiple distance metrics\n",
    "probe_directory = \"../simclr_resources/probe\"\n",
    "gallery_faiss_path_vggface2 = \"../storage/catalog/vggface2.index\"\n",
    "gallery_metadata_path_vggface2 = \"../storage/catalog/vggface2_metadata.pkl\"\n",
    "\n",
    "print(\"Evaluating vggface2 model on all probe images...\")\n",
    "\n",
    "# Initialize the 'vggface2' pipeline\n",
    "pipeline_vggface2 = Pipeline(pretrained='vggface2', index_type='IVF')\n",
    "\n",
    "# Loop through each distance metric and evaluate\n",
    "for metric in distance_metrics:\n",
    "    print(f\"Evaluating vggface2 with {metric} distance...\")\n",
    "    map_vggface2 = evaluate(pipeline_vggface2, probe_directory, gallery_faiss_path_vggface2, gallery_metadata_path_vggface2, k=10, metric=metric)\n",
    "    print(f\"Mean Average Precision (MAP) for vggface2 using {metric}: {map_vggface2}\")\n",
    "\n",
    "# Evaluate the 'casia-webface' model using all probe images and multiple distance metrics\n",
    "gallery_faiss_path_casia_webface = \"../storage/catalog/casia_webface.index\"\n",
    "gallery_metadata_path_casia_webface = \"../storage/catalog/casia_webface_metadata.pkl\"\n",
    "\n",
    "print(\"Evaluating casia-webface model on all probe images...\")\n",
    "\n",
    "# Initialize the 'casia-webface' pipeline\n",
    "pipeline_casia_webface = Pipeline(pretrained='casia-webface', index_type='IVF')\n",
    "\n",
    "# Loop through each distance metric and evaluate\n",
    "for metric in distance_metrics:\n",
    "    print(f\"Evaluating casia-webface with {metric} distance...\")\n",
    "    map_casia_webface = evaluate(pipeline_casia_webface, probe_directory, gallery_faiss_path_casia_webface, gallery_metadata_path_casia_webface, k=10, metric=metric)\n",
    "    print(f\"Mean Average Precision (MAP) for casia-webface using {metric}: {map_casia_webface}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating vggface2 model on all probe images...\n",
      "Evaluating vggface2 with a k value of 2...\n",
      "Mean Average Precision (MAP) for vggface2 using k=2: 0.5395395395395396\n",
      "Evaluating vggface2 with a k value of 5...\n",
      "Mean Average Precision (MAP) for vggface2 using k=5: 0.5537732176621064\n",
      "Evaluating vggface2 with a k value of 10...\n",
      "Mean Average Precision (MAP) for vggface2 using k=10: 0.5445760627407453\n",
      "Evaluating vggface2 with a k value of 15...\n",
      "Mean Average Precision (MAP) for vggface2 using k=15: 0.5402126574591302\n",
      "Evaluating vggface2 with a k value of 20...\n",
      "Mean Average Precision (MAP) for vggface2 using k=20: 0.5344032945343152\n",
      "Evaluating casia-webface model on all probe images...\n",
      "Evaluating casia-webface with a k value of 2...\n",
      "Mean Average Precision (MAP) for casia-webface using k=2: 0.1031031031031031\n",
      "Evaluating casia-webface with a k value of 5...\n",
      "Mean Average Precision (MAP) for casia-webface using k=5: 0.11490101212323434\n",
      "Evaluating casia-webface with a k value of 10...\n",
      "Mean Average Precision (MAP) for casia-webface using k=10: 0.1187389770723104\n",
      "Evaluating casia-webface with a k value of 15...\n",
      "Mean Average Precision (MAP) for casia-webface using k=15: 0.11935457768791104\n",
      "Evaluating casia-webface with a k value of 20...\n",
      "Mean Average Precision (MAP) for casia-webface using k=20: 0.11887778870278012\n"
     ]
    }
   ],
   "source": [
    "# Define the distance metrics to test\n",
    "k_values = [2, 5, 10, 15, 20]\n",
    "metric = 'euclidean'\n",
    "\n",
    "print(\"Evaluating vggface2 model on all probe images...\")\n",
    "\n",
    "# Loop through each distance metric and evaluate\n",
    "for k in k_values:\n",
    "    print(f\"Evaluating vggface2 with a k value of {k}...\")\n",
    "    map_vggface2 = evaluate(pipeline_vggface2, probe_directory, gallery_faiss_path_vggface2, gallery_metadata_path_vggface2, k=k, metric=metric)\n",
    "    print(f\"Mean Average Precision (MAP) for vggface2 using k={k}: {map_vggface2}\")\n",
    "\n",
    "print(\"Evaluating casia-webface model on all probe images...\")\n",
    "\n",
    "# Loop through each distance metric and evaluate\n",
    "for k in k_values:\n",
    "    print(f\"Evaluating casia-webface with a k value of {k}...\")\n",
    "    map_casia_webface = evaluate(pipeline_casia_webface, probe_directory, gallery_faiss_path_casia_webface, gallery_metadata_path_casia_webface, k=k, metric=metric)\n",
    "    print(f\"Mean Average Precision (MAP) for casia-webface using k={k}: {map_casia_webface}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transormation Analysis\n",
    "\n",
    "After evaluating both models on various image transformations, it’s clear that the models respond differently to changes in brightness, contrast, and blur, compared to their performance on the original probe images.\n",
    "\n",
    "### vggface2 Performance:\n",
    "- **Low Brightness**: The MAP dropped significantly to 0.2067 compared to the baseline of 0.54. This suggests that dimly lit images negatively affect the model's ability to retrieve relevant results.\n",
    "- **High Brightness**: Performance was even lower at 0.1531, indicating that overly bright images also pose a challenge for vggface2.\n",
    "- **Low Contrast**: Interestingly, vggface2 performed better with low-contrast images, achieving a MAP of 0.6078, even surpassing the baseline performance.\n",
    "- **High Contrast**: The MAP dropped again to 0.2258, showing that extreme contrast also impacts retrieval accuracy.\n",
    "- **Blur**: The MAP of 0.4868 indicates that blur does degrade performance, but not as severely as brightness or high contrast.\n",
    "\n",
    "### casia-webface Performance:\n",
    "- **Low Brightness**: The MAP for casia-webface dropped drastically to 0.0267, showing the model is highly sensitive to low-light conditions.\n",
    "- **High Brightness**: Performance slightly improved to 0.0620, but still far below the baseline.\n",
    "- **Low Contrast**: Casia-webface struggled with low-contrast images, with a MAP of 0.0917, which is better than the brightness transformations but still poor.\n",
    "- **High Contrast**: The MAP was 0.0447, indicating that both extremes in contrast negatively affect casia-webface.\n",
    "- **Blur**: Like vggface2, casia-webface saw a drop in performance, achieving a MAP of 0.0878 on blurred images.\n",
    "\n",
    "### Comparison to Baseline Performance\n",
    "Compared to the baseline results, vggface2 continues to outperform casia-webface across all transformations, but both models show a noticeable decline in performance under certain conditions. For vggface2, low contrast actually improved performance, while brightness transformations (both low and high) significantly harmed its accuracy. Casia-webface struggled with all transformations, performing especially poorly on brightness variations and blur, reflecting its overall weaker performance in face retrieval tasks.\n",
    "\n",
    "In conclusion, vggface2 remains the better option, but its sensitivity to brightness and blur suggests areas for improvement. For applications dealing with varied lighting or blurry images, additional preprocessing or training might be needed to improve robustness. Casia-webface, on the other hand, consistently underperforms, making it less suitable for this face recognition task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created transformed images in ../simclr_resources/probe_low_brightness\n",
      "Created transformed images in ../simclr_resources/probe_high_brightness\n",
      "Created transformed images in ../simclr_resources/probe_low_contrast\n",
      "Created transformed images in ../simclr_resources/probe_high_contrast\n",
      "Created transformed images in ../simclr_resources/probe_blur\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "def create_transformed_images(probe_directory, output_directory, transformation_type, factor):\n",
    "    \"\"\"\n",
    "    Apply a transformation to all images in the probe directory and save them in a new output directory.\n",
    "    \n",
    "    :param probe_directory: The directory containing the original probe images.\n",
    "    :param output_directory: The directory where the transformed images will be saved.\n",
    "    :param transformation_type: The type of transformation ('brightness', 'contrast', 'blur').\n",
    "    :param factor: The intensity of the transformation.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "    \n",
    "    for root, _, files in os.walk(probe_directory):\n",
    "        for file in files:\n",
    "            if file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "                image_path = os.path.join(root, file)\n",
    "                image = Image.open(image_path)\n",
    "                \n",
    "                # Apply the transformation\n",
    "                if transformation_type == 'brightness':\n",
    "                    enhancer = ImageEnhance.Brightness(image)\n",
    "                    transformed_image = enhancer.enhance(factor)\n",
    "                elif transformation_type == 'contrast':\n",
    "                    enhancer = ImageEnhance.Contrast(image)\n",
    "                    transformed_image = enhancer.enhance(factor)\n",
    "                elif transformation_type == 'blur':\n",
    "                    transformed_image = image.filter(ImageFilter.GaussianBlur(factor))\n",
    "                else:\n",
    "                    raise ValueError(\"Unknown transformation type\")\n",
    "                \n",
    "                # Create the output directory \n",
    "                relative_path = os.path.relpath(root, probe_directory)  # Get relative path within the probe folder\n",
    "                new_output_dir = os.path.join(output_directory, relative_path)  # Create same folder structure\n",
    "                if not os.path.exists(new_output_dir):\n",
    "                    os.makedirs(new_output_dir)\n",
    "                \n",
    "                # Save the transformed image in the new output directory\n",
    "                new_filename = os.path.join(new_output_dir, file)\n",
    "                transformed_image.save(new_filename)\n",
    "\n",
    "def apply_transformations_to_probe_images(probe_directory, output_base_directory):\n",
    "    \"\"\"\n",
    "    Apply different transformations to the probe images and save them in new directories.\n",
    "    \n",
    "    :param probe_directory: The directory containing the original probe images.\n",
    "    :param output_base_directory: The base directory where transformed images will be saved.\n",
    "    \"\"\"\n",
    "    transformations = [\n",
    "        {'type': 'brightness', 'name': 'low_brightness', 'factor': 0.5},\n",
    "        {'type': 'brightness', 'name': 'high_brightness', 'factor': 1.5},\n",
    "        {'type': 'contrast', 'name': 'low_contrast', 'factor': 0.5},\n",
    "        {'type': 'contrast', 'name': 'high_contrast', 'factor': 1.5},\n",
    "        {'type': 'blur', 'name': 'blur', 'factor': 2}, \n",
    "    ]\n",
    "    \n",
    "    for transform in transformations:\n",
    "        output_directory = os.path.join(output_base_directory, f\"probe_{transform['name']}\")\n",
    "        create_transformed_images(probe_directory, output_directory, transform['type'], transform['factor'])\n",
    "        print(f\"Created transformed images in {output_directory}\")\n",
    "\n",
    "# Define the original probe directory and apply transformations\n",
    "probe_directory = \"../simclr_resources/probe\"\n",
    "output_base_directory = \"../simclr_resources\"\n",
    "\n",
    "apply_transformations_to_probe_images(probe_directory, output_base_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 'vggface2' model on transformed probe images...\n",
      "Evaluating 'vggface2' on probe_low_brightness...\n",
      "Mean Average Precision (MAP) for 'vggface2' using probe_low_brightness: 0.20674013188673693\n",
      "Evaluating 'vggface2' on probe_high_brightness...\n",
      "Mean Average Precision (MAP) for 'vggface2' using probe_high_brightness: 0.153176232733111\n",
      "Evaluating 'vggface2' on probe_low_contrast...\n",
      "Mean Average Precision (MAP) for 'vggface2' using probe_low_contrast: 0.6078001839708189\n",
      "Evaluating 'vggface2' on probe_high_contrast...\n",
      "Mean Average Precision (MAP) for 'vggface2' using probe_high_contrast: 0.22580359459989083\n",
      "Evaluating 'vggface2' on probe_blur...\n",
      "Mean Average Precision (MAP) for 'vggface2' using probe_blur: 0.48680106820097996\n"
     ]
    }
   ],
   "source": [
    "# Transformed probe directories\n",
    "transformed_probes = ['probe_low_brightness', 'probe_high_brightness', 'probe_low_contrast', 'probe_high_contrast', 'probe_blur']\n",
    "\n",
    "# Metric and k value\n",
    "k = 10\n",
    "metric = 'euclidean'\n",
    "\n",
    "# Evaluate the 'vggface2' model on each transformed probe directory\n",
    "print(\"Evaluating 'vggface2' model on transformed probe images...\")\n",
    "\n",
    "for probe_folder in transformed_probes:\n",
    "    probe_directory = f\"../simclr_resources/{probe_folder}\"\n",
    "    print(f\"Evaluating 'vggface2' on {probe_folder}...\")\n",
    "    map_vggface2 = evaluate(pipeline_vggface2, probe_directory, gallery_faiss_path_vggface2, gallery_metadata_path_vggface2, k=k, metric=metric)\n",
    "    print(f\"Mean Average Precision (MAP) for 'vggface2' using {probe_folder}: {map_vggface2}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating 'casia-webface' model on transformed probe images...\n",
      "Evaluating 'casia-webface' on probe_low_brightness...\n",
      "Mean Average Precision (MAP) for 'casia-webface' using probe_low_brightness: 0.026674889174889176\n",
      "Evaluating 'casia-webface' on probe_high_brightness...\n",
      "Mean Average Precision (MAP) for 'casia-webface' using probe_high_brightness: 0.06208311486089267\n",
      "Evaluating 'casia-webface' on probe_low_contrast...\n",
      "Mean Average Precision (MAP) for 'casia-webface' using probe_low_contrast: 0.09177490983046545\n",
      "Evaluating 'casia-webface' on probe_high_contrast...\n",
      "Mean Average Precision (MAP) for 'casia-webface' using probe_high_contrast: 0.044724486391153076\n",
      "Evaluating 'casia-webface' on probe_blur...\n",
      "Mean Average Precision (MAP) for 'casia-webface' using probe_blur: 0.08786564342119899\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the 'casia-webface' model on each transformed probe directory\n",
    "print(\"\\nEvaluating 'casia-webface' model on transformed probe images...\")\n",
    "\n",
    "for probe_folder in transformed_probes:\n",
    "    probe_directory = f\"../simclr_resources/{probe_folder}\"\n",
    "    print(f\"Evaluating 'casia-webface' on {probe_folder}...\")\n",
    "    map_casia_webface = evaluate(pipeline_casia_webface, probe_directory, gallery_faiss_path_casia_webface, gallery_metadata_path_casia_webface, k=k, metric=metric)\n",
    "    print(f\"Mean Average Precision (MAP) for 'casia-webface' using {probe_folder}: {map_casia_webface}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
